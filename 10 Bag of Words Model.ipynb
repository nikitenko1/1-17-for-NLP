{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "50a97490",
      "metadata": {
        "id": "50a97490"
      },
      "source": [
        "<p style=\"font-family:Roboto; font-size: 28px; color: magenta\"> Python for NLP: Creating Bag of Words Model from Scratch</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23fb694d",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Bag of Words model is one of the three most commonly used word embedding approaches with TF-IDF and Word2Vec \n",
        "being the other two'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0277c210",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0277c210",
        "outputId": "bf1ca56f-0a03-405c-efcc-23197ee57d02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n Suppose we have a corpus with three sentences:\\n\\n\"I like to play football\"\\n\"Did you go outside to play tennis\"\\n\"John and I play tennis\"\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        " Suppose we have a corpus with three sentences:\n",
        "\n",
        "\"I like to play football\"\n",
        "\"Did you go outside to play tennis\"\n",
        "\"John and I play tennis\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24af06b2",
      "metadata": {},
      "source": [
        "<p style=\"font-family:Roboto; font-size: 22px; color: orange; text-decoration-line: overline; \"> Part: _1: Tokenize the Sentences</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61ca828f",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "The first step in this regard is to convert the sentences in our corpus into tokens or individual words\n",
        "Sentence 1\tSentence 2\tSentence 3\n",
        "I\t        Did\t        John\n",
        "like\t    you         and\n",
        "to\t        go\t        I\n",
        "play\t    outside\t    play\n",
        "football\tto\t    tennis\n",
        "            play\t\n",
        "            tennis\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4173f260",
      "metadata": {},
      "source": [
        "<p style=\"font-family:Roboto; font-size: 22px; color: orange; text-decoration-line: overline; \"> Part: _2: Create a Dictionary of Word Frequency</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5921839b",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "The next step is to create a dictionary that contains all the words in our corpus \n",
        "as keys and the frequency of the occurrence of the words as value\n",
        "Word\tFrequency\n",
        "I\t        2\n",
        "like\t    1\n",
        "to\t        2\n",
        "play\t    3\n",
        "football\t1\n",
        "Did\t        1\n",
        "you\t        1\n",
        "go\t        1\n",
        "outside\t    1\n",
        "tennis\t    2\n",
        "John\t    1\n",
        "and\t        1\n",
        "Let's sort our word frequency dictionary:\n",
        "Word\tFrequency\n",
        "play\t    3\n",
        "tennis\t    2\n",
        "to\t        2\n",
        "I\t        2\n",
        "football\t1\n",
        "Did\t        1\n",
        "you\t        1\n",
        "go\t        1\n",
        "outside \t1\n",
        "like\t    1\n",
        "John\t    1\n",
        "and     \t1\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0d34fe7",
      "metadata": {},
      "source": [
        "<p style=\"font-family:Roboto; font-size: 22px; color: orange; text-decoration-line: overline; \"> Part: _3: Creating the Bag of Words Model</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bbbe8dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''To create the bag of words model, we need to create a matrix'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6c157e1",
      "metadata": {},
      "source": [
        "<p style=\"font-family:Roboto; font-size: 28px; color: magenta\"> Bag of Words Model in Python</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8cc7ed5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk  \n",
        "import numpy as np  \n",
        "import random  \n",
        "import string\n",
        "\n",
        "import bs4 as bs  \n",
        "import urllib.request  \n",
        "import re  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c97806ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''In the first step, we will scrape the Wikipedia article on Natural Language Processing'''\n",
        "raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')  \n",
        "raw_html = raw_html.read()\n",
        "\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "# we filter the text within the paragraph text\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:  \n",
        "    article_text += para.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "41f42bf0",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''The next step is to split the corpus into individual sentences'''\n",
        "corpus = nltk.sent_tokenize(article_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9f870bff",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(corpus )):\n",
        "    # we first convert our text into lower case\n",
        "    corpus [i] = corpus [i].lower()\n",
        "    # then will remove the punctuation from our text\n",
        "    corpus [i] = re.sub(r'\\W',' ',corpus [i])\n",
        "    # We will remove the empty spaces from the text using regex\n",
        "    corpus [i] = re.sub(r'\\s+',' ',corpus [i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "601a4e49",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34\n"
          ]
        }
      ],
      "source": [
        "print(len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f9ab4c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "as an example george lakoff offers a methodology to build natural language processing nlp algorithms through the perspective of cognitive science along with the findings of cognitive linguistics 50 with two defining aspects ties with cognitive linguistics are part of the historical heritage of nlp but they have been less frequently addressed since the statistical turn during the 1990s \n"
          ]
        }
      ],
      "source": [
        "# Let's print one sentence from our corpus: \n",
        "print(corpus[30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f5290e9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''The next step is to tokenize the sentences in the corpus and create a dictionary \n",
        "that contains words and their corresponding frequencies in the corpus'''\n",
        "wordfreq = {}\n",
        "for sentence in corpus:\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    for token in tokens:\n",
        "        if token not in wordfreq.keys():\n",
        "            wordfreq[token] = 1\n",
        "        else:\n",
        "            wordfreq[token] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "06a2c0c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Let us filter down to the 200 most frequently occurring words. To do so, we can make use of Python's heap library.'''\n",
        "import heapq\n",
        "most_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a41960f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "'''The final step is to convert the sentences in our corpus into their corresponding vector representation'''\n",
        "sentence_vectors = []\n",
        "for sentence in corpus:\n",
        "    sentence_tokens = nltk.word_tokenize(sentence)\n",
        "    sent_vec = []\n",
        "    for token in most_freq:\n",
        "        if token in sentence_tokens:\n",
        "            sent_vec.append(1)\n",
        "        else:\n",
        "            sent_vec.append(0)\n",
        "    sentence_vectors.append(sent_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c28ef94e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [0, 0, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 0, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Our model is in the form of a list of lists. We can convert our model into matrix form using this script:'''\n",
        "sentence_vectors = np.asarray(sentence_vectors)\n",
        "sentence_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f85a61fa",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
